---
source_path: industrial_organizational_psychology_an_applied_approach.md
pages: n/a-n/a
chunk_id: 5244ebc141881821110d2efb5ba85054cf352154
title: industrial_organizational_psychology_an_applied_approach
---
that particular construct, even though all of the tests measured “honesty.” Construct validity is usually determined by correlating scores on a test with scores from other tests. Some of the other tests measure the same construct, whereas others do not. For example, suppose we have a test that measures knowl- edge of psychology. One hundred people are administered our Knowledge of Psychology Test as well as another psychology knowledge test, a test of reading ability, and a test of general intelligence. If our test really measures the construct we say it does—knowledge of psychology—it should correlate highly with the other test of psychology knowledge but not very highly with the other two tests. If our test correlates highest with the reading ability test, our test may be content valid (it contained psychology items), but not construct valid because scores on our test are based more on reading ability than on knowledge of psychology. Another method of measuring construct validity is known-group validity (Hattie & Cooksey, ). This method is not common and should be used only when other methods for measuring construct validity are not practical. With known-group validity, a test is given to two groups of people who are “known” to be different on the trait in question. For example, suppose we wanted to determine the validity of our new hon- esty test. The best approach might be a criterion validity study in which we would correlate our employees’ test scores with their dishonest behavior, such as steal- ing or lying. The problem is, how would we know who stole or who lied? We could ask them, but would dishonest people tell the truth? Probably not. Instead, we decide to validate our test by administering it to a group known as honest (priests) and to another group known as dishonest (criminals). After administering the test to both groups, we find that, sure enough, the priests score higher on honesty than do the convicts. Does this mean our test is valid? Not necessarily. It means that the test has known-group validity but not necessarily other types of validity. We do not know whether the test will predict employee theft (criterion validity), nor do we know whether it is even measur- ing honesty (construct validity). It is possible that the test is actually measuring another construct on which the two groups differ (e.g., intelligence). Because of these problems, the best approach to take with known-group validity is this: If the known groups do not differ on test scores, consider the test invalid. If scores do differ, one still cannot be sure of its validity. Even though known-group validity usually should not be used to establish test validity, it is important to understand because some test companies use known- group validity studies to sell their tests, claiming that the tests are valid. Personnel analyst Jeff Rodgers once was asked to evaluate a test his company was consider- ing for selecting bank tellers. The test literature sounded impressive, mentioning that the test was “backed by over  validity studies.” Rodgers was suspicious and requested copies of the studies. After several months of “phone calls and teeth pulling,” he obtained reports of the validity studies. Most of the studies used known-group methodology and compared the scores of groups such as monks and priests. Not one study involved a test of criterion validity to demonstrate that the test could actually predict bank teller performance. Thus, if you hear that a test is valid, it is important to obtain copies of the research reports. Choosing a Way to Measure Validity With three common ways of measuring validity, one might logically ask which of the methods is the “best” to use. As with most questions in psychology, the answer is that “it depends.” In this case, it depends on the situation as well as what the person conducting the validity study is trying to accomplish. If it is to decide whether the test will be a useful predictor of employee performance, then content validity will usually be used, and a criterion validity study also will be conducted if there are enough employees and if a good measure of job perfor- mance is available. In deciding whether content validity is enough, I advise organizations to use the “next-door neighbor rule.” That is, ask yourself, “If my next-door neighbor were on a jury and I had to justify the use of my test, would content validity be enough?” For example, suppose you conducted a job analysis of a clerical position and find that typing, filing, and answering the phone are the primary duties. So you purchase a standard typing test and a filing test. The link between these tests and the duties performed by our clerical worker is so obvious that a criterion validity study is prob- ably not essential to convince a jury of the validity of the two tests. However, suppose your job analysis of a police officer indicates that making decisions under pressure is an important part of the job. To tap this dimension, you choose the Gandy Critical Thinking Test. Because the link between your test and the ability to make decisions under pressure is not so obvious, you may need a criterion validity study. Why not always conduct a criterion validity study? After all, isn’t a significant validity coefficient better than sex? Having the significant validity coefficient is great. But the danger is in conducting the validity study. If you conduct a crite- rion validity study and do not get significance, that failure could be deadly if you are taken to court. To get a significant validity coefficient, many things have to go right. You need a good test, a good measure of performance, and a decent sample size. Furthermore, most validity coefficients are small (in the . to . range). Though assessment experts understand the utility of such small correlations, it can be difficult to convince a jury or governmental agencies to share your excite- ment after you explain that the range for a correlation coefficient is  to , you got a correlation of ., and your test explains % of the variance. Finally, a test itself can never be valid. When we speak of validity, we are speak- ing about the validity of the test scores as they relate to a particular job. A test may be a valid predictor of tenure for counselors but not of performance for shoe sales- people. Thus, when we say that a test is valid, we mean that it is valid for a particular job and a particular criterion. No test will ever be valid for all jobs and all criteria. evaluating selection techniques and decisions 213 214 chapter  Face Validity Although face validity is not one of the three major methods of determining test validity cited in the federal Uniform Guidelines on Employee Selection Proce- dures, it is still important. Face validity is the extent to which a test appears to be job related. This perception is important because if a test or its items do not appear valid, the test-takers and administrators will not have confidence in the results. If job applicants do not think a test is job related, their perceptions of its fairness decrease, as does their motivation to do well on the test (Hausknecht, Day, & Thomas, ). Likewise, if employees involved in a training session on interpersonal skills take a personality inventory and are given the results, they will not be motivated to change or to use the results of the inventory unless the personality profile given to them seems accurate. The importance of face validity has been demonstrated in a variety of research studies. For example, Chan, Schmitt, DeShon, Clause, and Delbridge () found that face-valid tests resulted in high levels of test-taking motivation, which in turn resulted in higher levels of test performance. Thus, face validity motivates applicants to do well on tests. Face-valid tests that are accepted by applicants decrease the chance of lawsuits (Rynes & Connerley, ), reduce the number of applicants dropping out of the employment process (Thornton, ), and increase the chance that an applicant will accept a job offer (Hoff Macan, Avedon, & Paese, ). The face validity and acceptance of test results can be increased by informing the applicants about how a test relates to job performance (Lounsbury, Bobrow, & Jensen, ) and by administering the test in a multimedia format (Richman- Hirsch, Olson-Buchanan, & Drasgow, ). Acceptance of test results also increases when applicants receive honest feedback about their test performance and are treated with respect by the test administrator (Gilliland, ). But just because a test has face validity does not mean it is valid (Jackson, O’Dell, & Olson, ). For example, have you ever read a personality description based on your astrological sign and found the description to be quite accurate? Does this mean astrological forecasts are accurate? Not at all. If you also have read a personality description based on a different astrological sign, you probably
