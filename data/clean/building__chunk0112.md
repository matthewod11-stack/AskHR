---
source_path: building.md
pages: n/a-n/a
chunk_id: 0728972252299486dd9d502beb897f98139d4170
title: building
---
# evals

More desirable evaluation tests are ones that are:

- Very detailed and specific • Fully automatable (consider using LLMs as a judge) • Higher in volume even if lower quality

Less desirable evaluations are:

- Open-ended • Are not automated and require human judgment • High quality but at a very low volume

23

The Anthropic Console features an Evaluation tool that allows you to test your prompts under various scenarios. The Evaluation tool offers several features to help you refine your prompts:

- Side-by-side comparison: Compare the outputs of two or more prompts to quickly see the impact of your changes.

- Quality grading: Grade response quality on a 5-point scale

to track improvements in response quality per prompt.

- Prompt versioning: Create new versions of your prompt and re-run the test suite to quickly iterate and improve results.

You can learn more about building strong evaluations with our evaluations guide or course on prompt evaluations.

“By optimizing Claude around our industry expertise and specific requirements, we anticipate measurable improvements that deliver high-quality results at even faster speeds. We’ve already seen positive results with Claude 3 Haiku, and fine-tuning will enable us to tailor AI assistance more precisely.”
