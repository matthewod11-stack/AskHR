---
source_path: R2006F-PDF-ENG.md
pages: n/a-n/a
chunk_id: debc1afad53761f006746b372821851b7b9213aa
title: R2006F-PDF-ENG
---
## highest quality. At that time only expensive single-lens reﬂex cameras

could take such photos, but Hubel thought that with a dual-lens design

and advanced computational-photography techniques, Apple could add

the capability in the iPhone. His idea aligned well with the camera

team’s stated purpose: “More people taking better images more of the

time.”

As the team worked to turn this idea into reality, several challenges

emerged. The ﬁrst attempts produced some amazing portrait pictures

but also a number of “failure cases” in which the algorithm was unable

to distinguish between the central object in sharp relief (a face, for

instance) and the background being blurred. For example, if a person’s

face was to be photographed from behind chicken wire, it was not

possible to construct an algorithm that would capture the chicken wire

to the side of the face with the same sharpness as the chicken wire in

front of it. The wire to the side would be as blurred as the background.

One might say, “Who cares about the chicken wire case? That’s

exceedingly rare.” But for the team, sidestepping rare or extreme

situations—what engineers call corner cases—would violate Apple’s

strict engineering standard of zero “artifacts,” meaning “any undesired

or unintended alteration in data introduced in a digital process by an

involved technique and/or technology.” Corner cases sparked “many

tough discussions” between the camera team and other teams involved,

recalls Myra Haggerty, the VP of sensor software and UX prototyping,

Copyright © 2020 Harvard Business School Publishing. All rights reserved.

13

This document is authorized for use only by Matt O'Donnell (matthew.od11@gmail.com). Copying or posting is an infringement of copyright. Please contact customerservice@harvardbusiness.org or 800-988-0886 for additional copies.
