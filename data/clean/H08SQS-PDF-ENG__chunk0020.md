## by the detail and conﬁdent tone of the AI’s answer. In post-experiment

discussions, some participants noted that ChatGPT provided a wealth of

data and reasoning that was so thorough and self-assured, it made their

own initial estimates seem inadequate by comparison.

This is a form of AI authority bias: Because the AI spoke in a conﬁdent,

analytical manner, users tended to give its suggestions a lot of weight,

sometimes more than they gave their own judgment. Essentially, the

medium’s authority (an advanced AI, sounding like an expert report)

boosted the credibility of an optimistic forecast.

- 3. Emotion (or lack thereof)

Humans have emotions and instincts that can act as a check on extreme

forecasts. An executive looking at a meteoric stock chart might feel a

tinge of wariness and an inner voice saying “If it’s at a peak, it could

crash.” In our sessions, we suspect that emotional caution played a role

in the peer discussions. People might voice doubts or fears (“This stock

feels bubbly; maybe we should rein it in a bit”).

ChatGPT, on the other hand, has no such emotion or intuition. It doesn’t

feel fear of heights the way a person might when seeing a price chart

Copyright © 2025 Harvard Business School Publishing. All rights reserved.

5

This document is authorized for use only by Matt O'Donnell (matthew.od11@gmail.com). Copying or posting is an infringement of copyright. Please contact customerservice@harvardbusiness.org or 800-988-0886 for additional copies.