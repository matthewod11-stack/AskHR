# Unit Weight

−1 +1 0

A study of the use of a biodata instrument created in one organization and used in  others found that the validity generalized across all organiza- tions (Carlson, Scullen, Schmidt, Rothstein, & Erwin, ). Thus, biodata may be more stable across time and locations than was earlier thought (Rothstein, Schmidt, Erwin, Owens, & Sparks, ; Schmidt & Rothstein, ).

The second criticism is that some biodata items may not meet the legal requirements stated in the federal Uniform Guidelines, which establish fair hiring methods. Of greatest concern is that certain biodata items might lead to racial or sexual discrimination. For example, consider the selection item “ distance from work.” Applicants who live close to work might get more points than applicants who live farther away. The item may lead to racial discrimination if the organization is located in a predominantly White area. Removal of such discriminatory items, however, should eliminate most legal problems while still allowing for significant predictive validity (Reilly & Chao, ).

Though biodata instruments are valid and no more prone to adverse impact than other selection methods, the fact that applicants view them and personality tests as being the least job-related selection methods (Smither, Reilly, Millsap, Pearlman, & Stoffey, ) may increase the chance of a lawsuit being filed, but not the chance of losing a lawsuit.

To make biodata instruments less disagreeable to critics, Gandy and Dye

() developed four standards to consider for each potential item:

. The item must deal with events under a person’s control (e.g., a person would have no control over birth order but would have control over the number of speeding tickets she received).

. The item must be job related. . The answer to the item must be verifiable (e.g., a question about how many jobs an applicant has had is verifiable, but a question about the applicant’s favorite type of book is not).

. The item must not invade an applicant’s privacy (asking why an

applicant quit a job is permissible; asking about an applicant’s sex life is usually not).

Even though these four standards eliminated many potential items, Gandy and Dye () still obtained a validity coefficient of .. Just as impressive as the high validity coefficient was that the biodata instrument showed good prediction for African Americans, Whites, and Hispanics.

The third criticism is that biodata can be faked, a charge that has been made against every selection method except work samples and ability tests. Research indicates that applicants do in fact respond to items in socially desirable ways (Stokes et al., ). To reduce faking, several steps can be taken including:

7 7

7

warning applicants of the presence of a lie scale (Kluger & Colella, ) using objective, verifiable items (Becker & Colquitt, ; Shaffer, Saunders, & Owens, ) asking applicants to elaborate on their answers or to provide examples (Schmitt & Kunce, ). For example, if the biodata question asked, “How many leadership positions did you have in high school,” the next part of the item would be, “List the position titles and dates of those leadership positions.”

By including bogus items—items that include an experience that does not actu- ally exist (e.g., Conducted a Feldspar analysis to analyze data)—attempts to fake

employee selection: references and testing

185

biodata can be detected (Kim, ). When including bogus items in a biodata instrument, it is important that the items be carefully researched to ensure that they don’t represent activities that might actually exist (Levashina, Morgeson, & Cam- pion, ). For example, a bogus item of, “Have you ever conducted a Paradox analysis of a computer system?” might be interpreted by an applicant as, “Have you ever used the computer program, Paradox?”

A study by Ramsay, Kim, Oswald, Schmitt, and Gillespie () provides an excellent example of how endorsing a bogus item might be due more to confusion than to actual lying. In their study, Ramsay et al. found that only  out of  sub- jects said they had operated a rhetaguard (this machine doesn’t exist), yet  of the  said they had resolved disputes by isometric analysis. Why the difference? It may be that most people know the word “isometric” and most people have resolved disputes. They may not be sure if they ever used an isometric analysis to resolve their dispute, so many indicated that they had. With the rhetaguard, no applicant would have heard of it so they were probably sure they had not used it. Interestingly, bright applicants tend not to fake biodata items as often as applicants lower in cognitive ability. But when they do choose to fake, they are better at doing it (Lavashina et al., ). Not surprisingly, applicants who fake biodata instruments are also likely to fake personality inventories and integrity tests (Carroll, ).