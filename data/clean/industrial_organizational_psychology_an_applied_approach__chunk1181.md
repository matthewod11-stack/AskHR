---
source_path: industrial_organizational_psychology_an_applied_approach.md
pages: n/a-n/a
chunk_id: 834576ccf40302fc966285f9cf9ff9e058b678f6
title: industrial_organizational_psychology_an_applied_approach
---
# High School

Bachelor’s Master’s

40

59 1

80

15 5

−40 +44 −4

A problem with creating a biodata instrument is sample size. To create a reli- able and valid biodata instrument, it is desirable to have data from hundreds of employees. For most organizations, however, such large sample sizes are difficult if not impossible to obtain. In creating a biodata instrument with a small sample, the risk of using items that do not really predict the criterion increases. This issue is important because most industrial psychologists advise that employees should be split into two samples when a biodata instrument is created: One sample, the derivation sample, is used to form the weights; the other sample, the hold-out sample, is used to double-check the selected items and weights. Although this sample splitting sounds like a great idea, it is not practical when dealing with a small or moderate sample size.

Research by Schmitt, Coyle, and Rauschenberger () suggests that there is less chance of error when a sample is not split. Discussion on whether to split samples is bound to continue in the years ahead, but because many HR profes- sionals will be dealing with relatively small numbers of employees, it might be best to create and validate a biodata instrument without splitting employees into derivation and hold-out samples.

A final issue to consider is the sample used to create the biodata instrument. Responses of current employees can be used to select the items and create the weights that will be applied to applicants. Stokes, Hogan, and Snell () found that incumbents and applicants respond in very different ways, indicating that the use of incumbents to create and scale items may reduce validity. To make the biodata process more clear, complete the biodata exercise in your CD-ROM.

Criticisms of Biodata Even though biodata does a good job of predicting future employee behavior, it has been criticized on two major points. The first holds that the validity of biodata may not be stable—that is, its ability to predict employee behavior decreases with time. For example, Wernimont () found that only three questions retained their predictive validity over the five-year period from  to . Similar results were reported by Hughes, Dunn, and Baxter ().

Other research (Brown, ), however, suggests that declines in valid- ity found in earlier studies may have resulted from small samples in the initial development of the biodata instrument. Brown used data from more than , life insurance agents to develop his biodata instrument, but data from only  agents were used to develop the biodata instrument that was earlier criticized by Wernimont (). Brown compared the validity of his original sample () with those from samples taken  years later () and  years later (). The results indicated that the same items that significantly predicted the criterion in  predicted at similar levels in .
