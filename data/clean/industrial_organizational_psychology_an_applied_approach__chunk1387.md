---
source_path: industrial_organizational_psychology_an_applied_approach.md
pages: n/a-n/a
chunk_id: 798bfebc1fdce2428f76f5d6c98868eb6899b515
title: industrial_organizational_psychology_an_applied_approach
---
# Average score

14.9

16.9

Any changes in a test potentially change its reliability, validity, difficulty, or all three. Such changes might include the order of the items, examples used in the questions, method of administration, and time limits. Though alternate-form differences potentially affect test outcomes, most research indicates that these effects are either nonexistent or rather small. For example, meta-analyses sug- gest that computer administration (Dwight & Feigelson, ; Mead & Drasgow, ) or PowerPoint administration (Larson, ) of cognitive ability tests results in scores equivalent to paper-and-pencil administration. However, a quasi-experimental study by Ployhart, Weekley, Holtz, and Kemp () found that personality inventories and situational judgment tests administered on the Web resulted in lower scores and better internal reliability than the same test administered in the traditional paper-and-pencil format. Interestingly, research suggests that African Americans, but not Whites, score higher on video-based tests than on traditional paper-and-pencil tests (Chan & Schmitt, ).

Internal Reliability A third way to determine the reliability of a test or inventory is to look at the consistency with which an applicant responds to items measuring a similar dimension or construct (e.g., personality trait, ability, area of knowledge). The extent to which similar items are answered in similar ways is referred to as internal consistency and measures item stability.

In general, the longer the test, the higher its internal consistency—that is, the agreement among responses to the various test items. To illustrate this point, let us look at the final exam for this course. If the final were based on three chapters, would you want a test consisting of only three multiple-choice items? Probably not. If you made a careless mistake in marking your answer or fell asleep during part of the lecture from which a question was taken, your score would be low. But if the test had  items, one careless mistake or one missed part of a lecture would not severely affect your total score.

Another factor that can affect the internal reliability of a test is item homogeneity. That is, do all of the items measure the same thing, or do they measure different constructs? The more homogeneous the items, the higher the internal consistency. To illustrate this concept, let us again look at your final exam based on three chapters.

If we computed the reliability of the entire exam, it would probably be relatively low. Why? Because the material assessed by the test questions is not homogeneous. They are measuring knowledge from three topic areas (three chapters), two sources (lecture and text), and two knowledge types (factual and conceptual). If we broke the test down by chapter, source, and item type, the reliability of the separate test components would be higher, because we would be looking at groups of homogeneous items.

When reading information about internal consistency in a journal arti- cle or a test manual, you will encounter three terms that refer to the method used to determine internal consistency: split-half, coefficient alpha, and K-R  (Kuder- Richardson formula ). The split-half method is the easiest to use, as items on a test are split into two groups. Usually, all of the odd-numbered items are in one group and all the even-numbered items are in the other group. The scores on the two groups of items are then correlated. Because the number of items in the test has been reduced, researchers have to use a formula called Spearman-Brown prophecy to adjust the correlation.

evaluating selection techniques and decisions

207

208

chapter 

Cronbach’s coefficient alpha (Cronbach, ) and the K-R  (Kuder & Richardson, ) are more popular and accurate methods of determining inter- nal reliability, although they are more complicated to use and thus are calculated by computer program rather than by hand. Essentially, both the coefficient alpha and the K-R  represent the reliability coefficient that would be obtained from all possible combinations of split halves. The difference between the two is that the K-R  is used for tests containing dichotomous items (e.g., yes/no, true/ false), whereas the coefficient alpha can be used not only for dichotomous items but for tests containing interval and ratio items such as five-point rating scales. The median internal reliability coefficient found in the research literature is ., and coefficient alpha is by far the most commonly reported measure of internal reliability (Hogan, Benjamin, & Brezinski, ).

Scorer Reliability A fourth way of assessing reliability is scorer reliability. A test or inventory can have homogeneous items and yield heterogeneous scores and still not be reli- able if the person scoring the test makes mistakes. Scorer reliability is an issue in projective or subjective tests in which there is no one correct answer, but even tests scored with the use of keys suffer from scorer mistakes. For example, Allard, Butler, Faust, and Shea () found that % of hand-scored personality tests contained at least one scoring error, and % contained enough errors to alter a clinical diagnosis. Goddard, Simons, Patton, and Sullivan () found that % of hand-scored interest inventories contained scoring or plotting errors, and of that percentage, % would have changed the career advice offered.

When human judgment of performance is involved, scorer reliability is discussed in terms of interrater reliability. That is, will two interviewers give an applicant similar ratings, or will two supervisors give an employee similar perfor mance ratings? If you are a fan of American Idol, how would you rate the interrater reliability among Simon, Paula, and Randy?

Evaluating the Reliability of a Test In the previous pages, you learned that it is important that scores on a test be reliable and that there are four common methods for determining reliability. When deciding whether a test demonstrates sufficient reliability, two factors must be considered: the magnitude of the reliability coefficient and the people who will be taking the test.

The reliability coefficient for a test can be obtained from your own data, the test manual, journal articles using the test, or test compendia that will be discussed later in the chapter. To evaluate the coefficient, you can compare it with reliability coefficients typically obtained for similar types of tests. For example, if you were considering purchasing a personality inventory and saw in the test manual that the test-retest reliability was ., a comparison with the coefficients shown in Table . would show that the reliability for the test you are considering is lower than what is normally found for that type of test.

The second factor to consider is the people who will be taking your test. For example, if you will be using the test for managers, but the reliability coefficient in the test manual was established with high school students, you would have less confidence that the reliability coefficient would generalize well to your organiza- tion. A good example of this was the meta-analysis of the reliability of the NEO personality scales. In that meta-analysis, Caruso () found that the reliability
