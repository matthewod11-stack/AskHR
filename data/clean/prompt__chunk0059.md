# Prompt engineering

Remember how an LLM works; it’s a prediction engine. The model takes sequential text as an input and then predicts what the following token should be, based on the data it was

trained on. The LLM is operationalized to do this over and over again, adding the previously

predicted token to the end of the sequential text for predicting the following token. The next

token prediction is based on the relationship between what’s in the previous tokens and what

the LLM has seen during its training.

When you write a prompt, you are a(cid:459)empting to set up the LLM to predict the right sequence

of tokens. Prompt engineering is the process of designing high-quality prompts that guide

LLMs to produce accurate outputs. This process involves tinkering to (cid:450)nd the best prompt,

optimizing prompt length, and evaluating a prompt’s writing style and structure in relation

to the task. In the context of natural language processing and LLMs, a prompt is an input

provided to the model to generate a response or prediction.