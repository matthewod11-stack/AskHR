---
source_path: prompt.md
pages: n/a-n/a
chunk_id: fc6391da180473327520bb475750e0a2ae8afe31
title: prompt
---
# higher costs.

Sampling controls

LLMs do not formally predict a single token. Rather, LLMs predict probabilities for what the

next token could be, with each token in the LLMâ€™s vocabulary ge(cid:459)ing a probability. Those

token probabilities are then sampled to determine what the next produced token will be.

Temperature, top-K, and top-P are the most common con(cid:450)guration se(cid:459)ings that determine

how predicted token probabilities are processed to choose a single output token.
