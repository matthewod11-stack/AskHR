---
source_path: Reinventing Performance Management.md
pages: n/a-n/a
chunk_id: eebc69e65b1638eb7b5b9f16879a7013a8fc9ba2
title: Reinventing Performance Management
---
## THE BIG IDEA REINVENTING PERFORMANCE MANAGEMENT of a team member (in a 360-degree or an upward- feedback survey, for example), we found that we will need to ask only the immediate team leader— but, critically, to ask a different kind of question. People may rate other people’s skills inconsistently, but they are highly consistent when rating their own feelings and intentions. To see performance at the individual level, then, we will ask team leaders not about the skills of each team member but about their own future actions with respect to that person. In the end, it’s not the particular number we assign to a person that’s the problem; rather, it’s the fact that there is a single number. - 4. This person is ready for promotion today [mea- sures potential on a yes-or-no basis]. In effect, we are asking our team leaders what they would do with each team member rather than what they think of that individual. When we aggre- gate these data points over a year, weighting each according to the duration of a given project, we produce a rich stream of information for leaders’ discussions of what they, in turn, will do—whether it’s a question of succession planning, develop- ment paths, or performance-pattern analysis. Once a quarter the organization’s leaders can use the new data to review a targeted subset of employees (those eligible for promotion, for example, or those with critical skills) and can debate what actions Deloitte might take to better develop that particular group. In this aggregation of simple but powerful data points, we see the possibility of shifting our 2-million-hour annual investment from talking about the ratings to talking about our people—from ascertaining the facts of performance to considering what we should do in response to those facts. At the end of every project (or once every quarter for long-term projects) we will ask team leaders to respond to four future- focused statements about each team member. We’ve refined the wording of these statements through successive tests, and we know that at Deloitte they clearly highlight differences among indi- viduals and reliably measure performance. Here are the four: 1. Given what I know of this person’s performance, and if it were my money, I would award this person the highest possible compensation increase and bo- nus [measures overall performance and unique value to the organization on a five-point scale from “strongly agree” to “strongly disagree”]. 2. Given what I know of this person’s per- formance, I would always want him or her on my team [measures ability to work well with others on the same five-point scale]. 3. This person is at risk for low perfor- mance [identifies problems that might harm the customer or the team on a yes- or-no basis]. In addition to this consistent—and countable— data, when it comes to compensation, we want to factor in some uncountable things, such as the dif- ficulty of project assignments in a given year and contributions to the organization other than formal projects. So the data will serve as the starting point for compensation, not the ending point. The final de- termination will be reached either by a leader who knows each individual personally or by a group of leaders looking at an entire segment of our practice and at many data points in parallel. We could call this new evaluation a rating, but it bears no resemblance, in generation or in use, to the ratings of the past. Because it allows us to quickly capture performance at a single moment in time, we call it a performance snapshot. The Third Objective Two objectives for our new system, then, were clear: We wanted to recognize performance, and we had to be able to see it clearly. But all our research, all our conversations with leaders on the topic of perfor- mance management, and all the feedback from our people left us convinced that something was miss- ing. Is performance management at root more about “management” or about “performance”? Put differ- ently, although it may be great to be able to measure and reward the performance you have, wouldn’t it be better still to be able to improve it? 8 Harvard Business Review April 2015 This document is authorized for use only by Matt O'Donnell (matthew.od11@gmail.com). Copying or posting is an infringement of copyright. Please contact customerservice@harvardbusiness.org or 800-988-0886 for additional copies. Our third objective therefore became to fuel per- formance. And if the performance snapshot was an organizational tool for measuring it, we needed a tool that team leaders could use to strengthen it. FOR ARTICLE REPRINTS CALL 800-988-0886 OR 617-783-7500, OR VISIT HBR.ORG How Deloitte Built a Radically Simple Performance Measure One of the most important tools in our redesigned performance management system is the “performance snapshot.” It lets us see performance quickly and reliably across the organization, freeing us to spend more time engaging with our people. Here’s how we created it. Research into the practices of the best team lead- ers reveals that they conduct regular check-ins with each team member about near-term work. These brief conversations allow leaders to set expecta- tions for the upcoming week, review priorities, comment on recent work, and provide course cor- rection, coaching, or important new information. The conversations provide clarity regarding what is expected of each team member and why, what great work looks like, and how each can do his or her best work in the upcoming days—in other words, exactly the trinity of purpose, expectations, and strengths that characterizes our best teams. Our design calls for every team leader to check in with each team member once a week. For us, these check-ins are not in addition to the work of a team leader; they are the work of a team leader. If a leader checks in less often than once a week, the team member’s priorities may become vague and aspira- tional, and the leader can’t be as helpful—and the conversation will shift from coaching for near-term work to giving feedback about past performance. In other words, the content of these conversations will be a direct outcome of their frequency: If you want people to talk about how to do their best work in the near future, they need to talk often. And so far we have found in our testing a direct and measurable correlation between the frequency of these conver- sations and the engagement of team members. Very frequent check-ins (we might say radically frequent check-ins) are a team leader’s killer app. That said, team leaders have many demands on their time. We’ve learned that the best way to ensure frequency is to have check-ins be initiated by the team member—who more often than not is eager for the guidance and attention they provide—rather than by the team leader. To support both people in these conversations, our system will allow individual members to un- derstand and explore their strengths using a self- assessment tool and then to present those strengths to their teammates, their team leader, and the rest of the organization. Our reasoning is twofold. First, as we’ve seen, people’s strengths generate their highest performance today and the greatest improvement in their performance tomorrow, and so deserve to be a central focus. Second, if we want to see frequent 1 THE CRITERIA We looked for measures that met three criteria. To neutralize the idiosyncratic rater eﬀect, we wanted raters to rate their own actions, rather than the qualities or behaviors of the ratee. To generate the necessary range, the questions had to be phrased in the extreme. And to avoid confusion, each one had to contain a single, easily understood concept. We chose one about pay, one about teamwork, one about poor performance, and one about promotion. Those categories may or may not be right for other organizations, but they work for us. 2 THE RATER We were looking for someone with vivid experience of the individual’s performance and whose subjective judgment we felt was important. We agreed that team leaders are closest to the performance of ratees and, by virtue of their roles, must exercise subjective judgment. We could have included functional managers, or even ratees’ peers, but we wanted to start with clarity and simplicity. 3 TESTING We then tested that our questions would produce useful data. Validity testing focuses on their diﬃculty (as revealed by mean responses) and the range of responses (as revealed by standard deviations). We knew that if they consistently yielded a tight cluster of “strongly agree” responses, we wouldn’t get the diﬀerentiation we were looking for. Construct validity and criterion-related validity are also important. (That is, the questions should collectively test an underlying theory and make it possible to ﬁnd correlations with outcomes measured in other ways, such as engagement surveys.) 4 FREQUENCY At Deloitte we live and work in a project structure, so it makes sense for us to produce a performance snapshot at the end of each project. For longer-term projects we’ve decided that quarterly is
