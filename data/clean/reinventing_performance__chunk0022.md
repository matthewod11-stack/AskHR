# HBR.ORG

How Deloitte Built a Radically Simple Performance Measure One of the most important tools in our redesigned performance management system is the “performance snapshot.” It lets us see performance quickly and reliably across the organization, freeing us to spend more time engaging with our people. Here’s how we created it.

1 THE CRITERIA We looked for measures that met three criteria. To neutralize the idiosyncratic rater eﬀect, we wanted raters to rate their own actions, rather than the qualities or behaviors of the ratee. To generate the necessary range, the questions had to be phrased in the extreme. And to avoid confusion, each one had to contain a single, easily understood concept. We chose one about pay, one about teamwork, one about poor performance, and one about promotion. Those categories may or may not be right for other organizations, but they work for us.

questions should collectively test an underlying theory and make it possible to ﬁnd correlations with outcomes measured in other ways, such as engagement surveys.)

4 FREQUENCY At Deloitte we live and work in a project structure, so it makes sense for us to produce a performance snapshot at the end of each project. For longer-term projects we’ve decided that quarterly is the best frequency. Our goal is to strike the right balance between tying the evaluation as tightly as possible to the experience of the performance and not overburdening our team leaders, lest survey fatigue yield poor data.

2 THE RATER We were looking for someone with vivid experience of the individual’s performance and whose subjective judgment we felt was important. We agreed that team leaders are closest to the performance of ratees and, by virtue of their roles, must exercise subjective judgment. We could have included functional managers, or even ratees’ peers, but we wanted to start with clarity and simplicity.

3 TESTING We then tested that our questions would produce useful data. Validity testing focuses on their diﬃculty (as revealed by mean responses) and the range of responses (as revealed by standard deviations). We knew that if they consistently yielded a tight cluster of “strongly agree” responses, we wouldn’t get the diﬀerentiation we were looking for. Construct validity and criterion-related validity are also important. (That is, the

5 TRANSPARENCY We’re experimenting with this now. We want our snapshots to reveal the real-time “truth” of what our team leaders think, yet our experience tells us that if they know that team members will see every data point, they may be tempted to sugarcoat the results to avoid diﬃcult conversations. We know that we’ll aggregate an individual’s snapshot scores into an annual composite. But what, exactly, should we share at year’s end? We want to err on the side of sharing more, not less—to aggregate snapshot scores not only for client work but also for internal projects, along with performance metrics such as hours and sales, in the context of a group of peers—so that we can give our people the richest possible view of where they stand. Time will tell how close to that ideal we can get.