---
source_path: prompt.md
pages: n/a-n/a
chunk_id: 5538c40377a5cd04a3ca72885309a55a6a7322ba
title: prompt
---
# somewhat intuitive.

The LLM response includes the chain of thought reasoning, which means more output

tokens, which means predictions cost more money and take longer.

To explain the following example in Table 11, letâ€™s (cid:450)rst try to create a prompt that is not using

CoT prompting to showcase the (cid:453)aws of a large language model.
