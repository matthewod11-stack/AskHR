being the product of chance. They say nothing about the strength of the results. Thus, a study with results significant at the . level does not necessar- ily show a stronger effect than a study with results significant at the . level of confidence. To determine the strength of a finding, we use the effect size, discussed ear- lier in the section on meta-analysis. Significance levels tell us the statistical sig- nificance of a study, and effect sizes (combined with logic) tell us the practical significance of a study. For example, suppose we conduct a study comparing the SAT scores of male and female high school students. Based on a sample of  million students, we find that males average , and females ,. With such a huge sample size, we will probably find that the two means are statistically different. However, with only a -point difference between the two groups on a test with a maximum score of ,, we would probably not place much practical significance in the difference. Correlation. It is necessary to discuss one particular statistic—correlation—because it is so widely used in I/O psychology and throughout this book. Correlation is a statistical procedure that enables a researcher to determine the relation- ship between two variables—for example, the relationships found between an employment test and future employee performance; job satisfaction and job attendance; or performance ratings made by workers and supervisors. It is important to understand that correlational analysis does not necessarily say any- thing about causality. Why does a correlation coefficient not indicate a cause-and-effect relation- ship? Because a third variable, an intervening variable, often accounts for the relationship between two variables. Take the example often used by psycholo- gist David Schroeder. Suppose there is a correlation of +. between the number of ice cream cones sold in New York during August and the number of babies that die during August in India. Does eating ice cream kill babies in another nation? No, that would not make sense. Instead, we look for that third variable that would explain our high correlation. In this case, the answer is clearly the summer heat. Another interesting example was provided by Mullins () in a presentation about the incorrect interpretation of correlation coefficients. Mullins pointed out that data show a strong negative correlation between the number of cows per square mile and the crime rate. With his tongue firmly planted in his cheek, Mullins suggested that New York City could rid itself of crime by importing millions of heads of cattle. Of course, the real interpretation for the negative correlation is that crime is greater in urban areas than in rural areas. A good researcher should always be cautious about variables that seem related. Several years ago, People magazine reported on a minister who conducted a “study” of  pregnant teenage girls and found that rock music was being played when  of them became pregnant. The minister concluded that because the two are related (i.e., they occurred at the same time) rock music must cause pregnancy. His solution? Outlaw rock music, and teenage pregnancy would dis- appear. In my own “imaginary study,” however, I found that in all  cases of teenage pregnancy, a pillow also was present. To use the same logic as that used by the minister, the real solution would be to outlaw pillows, not rock music. Although both “solutions” are certainly strange, the point should be clear: Just because two events occur at the same time or seem to be related does not mean that one event or variable causes another. The result of correlational analysis is a number called a correlation coef- ficient. The values of this coefficient range from − to +; the further the coefficient is from zero, the greater the relationship between two variables. That is, a correlation of . shows a stronger relationship between two variables than a correlation of .. Likewise, a correlation of −. shows a stronger rela- tionship than a correlation of +.. The + and − signs indicate the direction of the correlation. A positive (+) correlation means that as the values of one vari- able increase, so do the values of a second variable. For example, we might find a positive correlation between intelligence and scores on a classroom exam. This would mean that the more intelligent the student, the higher her score on the exam. A negative (−) correlation means that as the values of one variable increase, the values of a second variable decrease. For example, we would probably find a negative correlation between the number of beers that you drink the night before a test and your score on that test. In I/O psychology, we find negative correlations between job satisfaction and absenteeism, age and reaction time, and nervousness and interview success. To put together everything you have learned about research in Chapter ,