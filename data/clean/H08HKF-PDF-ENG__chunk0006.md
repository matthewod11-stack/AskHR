## “AI ethical risk,” and “AI governance” programs), from healthcare and

pharmaceuticals to insurance, ecommerce, food and beverage, mining,

and more. In doing so, we’ve noticed a pattern: Companies are rushing

to implement their RAI programs before they’ve ﬁnished designing

them. The results are predictable: ineﬃcient and diﬃcult-to-scale eﬀorts at managing AI’s ethical, reputational, and legal risks; wasted

resources; and slowed innovation.

It’s understandable how this happens. Before leaders are ready to invest

resources in an RAI program, they want to know whether AI even works

for the intended use case, so they put RAI to the side. No one wants to

invest in seat belts if they don’t know if the car can move. Then, when

they ﬁgure out that some AI solutions actually work and there’s real ROI

on the table, the risks they brushed to the side become urgent problems.

Leaders rush to implement RAI programs without having fully thought through whether they’re eﬃcient and eﬀective. Because many simply

don’t know what a robust RAI program looks like, teams are often left

trying to ﬁgure it out on the ﬂy.

One seemingly reasonable solution is to develop what one might call

a mini RAI program aimed at just the speciﬁc solutions that will be

deployed. There might be use case- or solution-speciﬁc RAI policies

for usage, risk assessments, metrics, and so on. This is a ﬁne stop-gap

measure, but eventually these mini RAI programs become unwieldy and

ineﬃcient, and things will fall between the cracks. At this point, if not

sooner, designing and implementing an enterprise-wide RAI program is