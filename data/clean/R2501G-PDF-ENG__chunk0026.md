## launch experiments with minimal input from data scientists.

Automated rollbacks. These are quantitative criteria that act as trip

wires to stop an experiment if its impact is too negative—for example,

a signiﬁcant drop in the number of daily active users of a social

media site. The impact is measured using guardrail metrics—secondary

measurements that ensure that while you’re focused on improving one

outcome, you don’t unintentionally harm other important areas such as

user experience, revenue, or system stability. When a vast number of

experiments are running concurrently, such a feature is vital.

An AI assistant that provides easy-to-understand explanations of complex

concepts. This core element can simplify the design and analysis of

experiments, making the process accessible even to novice users.

Data scientists’ role. In addition to setting up the platform, data

scientists should be responsible for training employees, creating the

materials for that training, and holding oﬃce hours to answer complex

questions after everyone is up and running. The time they spend on

most tests will drop to nearly zero because they will no longer be

involved in execution or analysis. (They will still be involved in novel

tests, such as the ﬁrst in a new product space, and will be called in when

results are challenging to interpret. But those are the exceptions.) Thus

they can focus on projects of greater impact that leverage their unique

expertise: for example, developing new statistical methods for analyzing

Copyright © 2025 Harvard Business School Publishing. All rights reserved.

6

This document is authorized for use only by Matt O'Donnell (matthew.od11@gmail.com). Copying or posting is an infringement of copyright. Please contact customerservice@harvardbusiness.org or 800-988-0886 for additional copies.