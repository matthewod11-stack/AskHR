---
source_path: industrial_organizational_psychology_an_applied_approach.md
pages: n/a-n/a
chunk_id: 7d3b61f031ee34ed1b622aa5f39556cbf1259d95
title: industrial_organizational_psychology_an_applied_approach
---
# S Have you conducted validity studies? The vendor should make these available to you. You want to see criterion validity studies related to your particular job or type of organization. Do scores on the test have adverse impact on women or minorities? If they do, it doesn’t mean you can’t use the test. Instead it increases the need for good validity studies and also lets you know that you could be in for a legal challenge. Has the test been challenged in court? What were the results? must constitute a representative sample of the material contained in the three chapters; therefore, there should be some  questions from each chapter. If there are  questions each from Chapters  and , the test will not be content valid because it left out Chapter . Likewise, if there are questions from Chapter , the test will not be content valid because it requires knowledge that is outside of the appropriate domain. In industry, the appropriate content for a test or test battery is determined by the job analysis. A job analysis should first determine the tasks and the con- ditions under which they are performed. Next the KSAOs (knowledge, skills, abilities, and other characteristics) needed to perform the tasks under those particular circumstances are determined. All of the important dimensions identified in the job analysis should be covered somewhere in the selection pro- cess, at least to the extent that the dimensions (constructs) can be accurately and realistically measured. Anything that was not identified in the job analysis should be left out. The readability of a test is a good example of how tricky content validity can be. Suppose we determine that conscientiousness is an important aspect of a job. We find a personality inventory that measures conscientiousness, and we are confident that our test is content valid because it measures a dimension identi- fied in the job analysis. But the personality inventory is very difficult to read (e.g., containing such words as meticulous, extraverted, gregarious) and most of our applicants are only high school graduates. Is our test content valid? No, because it requires a high level of reading ability, and reading ability was not identified as an important dimension for our job. Criterion Validity Another measure of validity is criterion validity, which refers to the extent to which a test score is related to some measure of job performance called a criterion (criteria will be discussed more thoroughly in Chapter ). Commonly used criteria include supervisor ratings of performance, actual measures of performance (e.g., sales, number of complaints, number of arrests made), attendance (tardiness, absenteeism), tenure, training performance (e.g., police academy grades), and discipline problems. 210 chapter  Criterion validity is established using one of two research designs: concur- rent or predictive. With a concurrent validity design, a test is given to a group of employees who are already on the job. The scores on the test are then correlated with a measure of the employees’ current performance. With a predictive validity design, the test is administered to a group of job applicants who are going to be hired. The test scores are then compared with a future measure of job performance. In the ideal predictive validity situation, every applicant (or a random sample of applicants) is hired, and the test scores are hidden from the people who will later make performance evaluations. If every applicant is hired, a wide range of both test scores and employee performance is likely to be found, and the wider the range of scores, the higher the validity coefficient. But because it is rarely practical to hire every applicant, the ideal predictive design is not often used. Instead, most criterion validity studies use a concurrent design. Why is a concurrent design weaker than a predictive design? The answer lies in the homogeneity of performance scores. In a given employment situation, very few employees are at the extremes of a performance scale. Employees who would be at the bottom of the performance scale either were never hired or have since been terminated. Employees who would be at the upper end of the performance scale often get promoted. Thus, the restricted range of performance scores makes obtaining a significant validity coefficient more difficult. A major issue concerning the criterion validity of tests focuses on a concept known as validity generalization, or VG—the extent to which a test found valid for a job in one location is valid for the same job in a different location. It was previously thought that the job of typist in one company was not the same as that in another company, the job of police officer in one small town was not the same as that in another small town, and the job of retail store supervisor was not the same as that of supervisor in a fast-food restaurant. In the past three decades, research has indicated that a test valid for a job in one organization is also valid for the same job in another organization (e.g., Schmidt, Gast-Rosenberg, & Hunter, ; Schmidt & Hunter, ; Schmidt, Hunter, Pearlman, & Hirsh, ). Schmidt, Hunter, and their associates have tested hundreds of thousands of employees to arrive at their conclusions. They suggest that previous thinking resulted from studies with small sample sizes, and test validity in one location but not another was the product primarily of sam- pling error. With large sample sizes, a test found valid in one location probably will be valid in another, providing that the jobs actually are similar and are not merely two separate jobs sharing the same job title. The two building blocks for validity generalization are meta-analysis, discussed in Chapter , and job analysis, discussed in Chapter . Meta-analysis can be used to determine the average validity of specific types of tests for a variety of jobs. For example, several studies have shown that cognitive ability is an excellent predictor of police performance. If we were to conduct a meta-analysis of all the studies looking at this relationship, we would be able to determine the average validity of cognitive ability in predicting police performance. If this validity coefficient is significant, then police departments similar to those used in the meta-analysis could adopt the test without conducting criterion validity studies of their own. This would be especially useful for small departments that have neither the number of officers necessary to properly conduct criterion validity studies nor the financial resources necessary to hire professionals to conduct such studies. Validity gener- alization should be used only if a job analysis has been conducted, the results of which show that the job in question is similar to those used in the meta-analysis. evaluating selection techniques and decisions 211 212 chapter  Construct Validity Construct validity is the most theoretical of the validity types. Basically, it is defined as the extent to which a test actually measures the construct that it purports to measure. Construct validity is concerned with inferences about test scores, in contrast to content validity, which is concerned with inferences about test construction. Perhaps a good example of the importance of construct validity is a situation I encountered during graduate school. We had just completed a job analysis of the entry-level police officer position for a small town. One of the important dimen- sions (constructs) that emerged was honesty. Almost every officer insisted that a good police officer was honest, so we searched for tests that measured honesty and quickly discovered that there were many types of honesty—a conclusion also reached by Rieke and Guastello (). Some honesty tests measured theft, some cheating, and others moral judgment. None measured the honesty construct as it was defined by these police officers: not taking bribes and not letting friends get away with crimes. No test measured that particular construct, even though all of the tests measured “honesty.” Construct validity is usually determined by correlating scores on a test with scores from other tests. Some of the other tests measure the same construct, whereas others do not. For example, suppose we have a test that measures knowl- edge of psychology. One hundred people are administered our Knowledge of Psychology Test as well as another psychology knowledge test, a test of reading ability, and a test of general intelligence. If our test really measures the construct we say it does—knowledge of psychology—it should correlate highly with the other test of psychology knowledge but not very highly with the other two tests. If our test correlates highest with the reading ability test, our test may be content valid (it contained psychology items), but not construct valid because scores on our test are based more on reading ability than on knowledge of psychology. Another method of measuring construct validity is known-group validity (Hattie & Cooksey, ). This method is not common and should be used only when other methods for measuring construct validity are not practical. With known-group validity, a test is given to two groups of people who are
