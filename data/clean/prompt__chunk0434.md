---
source_path: prompt.md
pages: n/a-n/a
chunk_id: 982d4e088be6d7ff099f709a6238f62d96e40f7f
title: prompt
---
# Prompt Engineering

Control the max token length

To control the length of a generated LLM response, you can either set a max token limit in the

con(cid:450)guration or explicitly request a speci(cid:450)c length in your prompt. For example:

"Explain quantum physics in a tweet length message."
