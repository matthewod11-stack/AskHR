# somewhat intuitive.

The LLM response includes the chain of thought reasoning, which means more output

tokens, which means predictions cost more money and take longer.

To explain the following example in Table 11, letâ€™s (cid:450)rst try to create a prompt that is not using

CoT prompting to showcase the (cid:453)aws of a large language model.