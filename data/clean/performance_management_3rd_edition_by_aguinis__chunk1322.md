---
source_path: performance_management_3rd_edition_by_aguinis.md
pages: n/a-n/a
chunk_id: 39ea91b74509ea3b44b34af8068766819b8dc0af
title: performance_management_3rd_edition_by_aguinis
---
## Chapter 7 • Implementing a Performance Management System 191

impression, (8) spillover, (9) stereotype, and (10) attribution. Unintentional errors can be minimized by implement- ing rater training programs.

- Rater error training (RET) exposes raters to the different errors and their causes. RET does not guarantee rating accuracy, but becoming aware of what types of errors are likely to occur and the reasons for these errors is a very good first step in minimizing them. • Frame of reference (FOR) training familiarizes raters with the various performance dimensions to be assessed. The goal is that raters will develop a com- mon FOR in observing and evaluating performance. This type of training is most appropriate when performance measurement focuses on behaviors.

- Behavioral observation (BO) training focuses on how raters observe, store, recall, and use information about performance. For example, this program teaches raters how to use aids such as diaries to standardize performance observation. This type of training is most appropriate when performance meas- urement focuses on counting and recording how frequently certain beha- viors and results take place.

- Self-leadership (SL) training aims at improving raters’ confidence in their ability to manage performance. SL training includes positive self-talk, mental imagery, and positive beliefs and thought patterns.

- Pilot testing the system before it is insti- tuted fully is useful because it allows potential problems and glitches to be discovered and corrective action to be taken before the system is put in place. Pilot testing consists of imple- menting the entire system, including all of its components, but only with a select group of people. Results are not recorded in employees’ records. Instead, the goal is that the people

participating in the pilot test provide feedback on any possible problems and on how to improve the system.

- The group participating in the pilot test needs to understand that the test will take time and resources. A representa- tive group should be selected so that conclusions drawn from the group can be generalized to the organization as a whole. The group should not be regarded as an exception in either a positive or negative way.

- After the system has been implemented, there should be a measurement system to evaluate the extent to which it is working the way it should and produc- ing the results that were expected. Such measures include confidential employee surveys assessing perceptions and attitudes about the system and whether there is an upward trend in perform- ance scores over time. Other measures include number of individuals evalu- ated, distribution of performance ratings, quality of performance informa- tion gathered, quality of performance discussion meetings, user satisfaction with the system, overall cost/benefit ratio, and unit- and organization-level performance indicators. Taken together, these indicators are a powerful tool that can be used to demonstrate the value of the performance management system. • Taking advantage of online applications can help speed up processes, lower cost, and gather and disseminate information faster and more effectively. Thus, online implementation of performance man- agement can make a good system even better. On the other hand, systems that are not implemented following best practices will not necessarily improve by using online components. In fact, online implementation may lead to highly undesirable outcomes: a more compli- cated system that is simply a big waste of time and resources for all those involved.
