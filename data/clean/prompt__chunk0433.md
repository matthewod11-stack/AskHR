# Prompt Engineering

Control the max token length

To control the length of a generated LLM response, you can either set a max token limit in the

con(cid:450)guration or explicitly request a speci(cid:450)c length in your prompt. For example:

"Explain quantum physics in a tweet length message."