---
source_path: prompt.md
pages: n/a-n/a
chunk_id: a68542d1af6778c0db86d583800ea36b2279e9f0
title: prompt
---
# with a temperature of 0.

NOTE: With more freedom (higher temperature, top-K, top-P, and output tokens), the LLM

might generate text that is less relevant.

WARNING: Have you ever seen a response ending with a large amount of (cid:450)ller words? This is also known as the "repetition loop bug", which is a common issue in Large Language

Models where the model gets stuck in a cycle, repeatedly generating the same ((cid:450)ller) word,

phrase, or sentence structure, o(cid:454)en exacerbated by inappropriate temperature and top-k/
