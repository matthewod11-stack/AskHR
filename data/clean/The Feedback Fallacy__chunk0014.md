## HBR / Magazine Article / The Feedback Fallacy

in the world: random error, which you can reduce by averaging many

readings; and systematic error, which you can’t. Unfortunately, we all

seem to have left math class remembering the former and not the latter.

We’ve built all our performance and leadership feedback tools as though

assessment errors are random, and they’re not. They’re systematic.

Consider color blindness. If we ask a color-blind person to rate the

redness of a particular rose, we won’t trust his feedback—we know

that he is incapable of seeing, let alone “rating,” red. His error isn’t

random; it’s predictable and explainable, and it stems from a ﬂaw in

his measurement system; hence, it’s systematic. If we then decide to

ask seven more color-blind people to rate the redness of our rose, their

errors will be equally systematic, and averaging their ratings won’t get

us any closer to determining the actual redness of the rose. In fact, it’s

worse than this. Adding up all the inaccurate redness ratings—“gray,”

“pretty gray,” “whitish gray,” “muddy brown,” and so on—and averaging

them leads us further away both from learning anything reliable about

the individuals’ personal experiences of the rose and from the actual

truth of how red our rose really is.

What the research has revealed is that we’re all color-blind when it

comes to abstract attributes, such as strategic thinking, potential, and

political savvy. Our inability to rate others on them is predictable and

explainable—it is systematic. We cannot remove the error by adding

more data inputs and averaging them out, and doing that actually

makes the error bigger.

Worse still, although science has long since proven that we are color-

blind, in the business world we assume we’re clear-eyed. Deep down

we don’t think we make very many errors at all. We think we’re reliable

raters of others. We think we’re a source of truth. We aren’t. We’re a